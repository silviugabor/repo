# spark://127.0.0.1:7077 or local
# local[n] is a special value that runs Spark on n threads on the local machine,
# without connecting to a cluster.
spark.master=local[*]

# Spark application name.
spark.application-name=Spark MLlib for Repositories

# Path to distributed library that should be loaded into each worker of a Spark cluster.
spark.distributed-libraries=<spark distributed libraries path should be correct path used on different environments>

# Amount of memory to use for the driver process.
spark.driver.memory=4g

# The maximum amount of CPU cores to request for the application from across the cluster (not from each machine).
spark.cores.max=8

# Amount of memory to assign to each executor process
spark.executor.memory=4g

# The largest number of partitions in a parent RDD during distributed shuffle operations.
# For local mode should be equal to number of cores on the local machine.
spark.default.parallelism=4

# Serializer: org.apache.spark.serializer.JavaSerializer (default) or org.apache.spark.serializer.KryoSerializer
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
spark.kryoserializer.buffer.max=256m

# The number of partitions to use when shuffling data for joins or aggregations.
spark.sql.shuffle.partitions=8

# MLlib JDBC data to read training sets from
repository.jdbc.url=jdbc:mysql://localhost:3306/licenta?unicode=true&characterEncoding=utf-8&autoReconnect=true&useSSL=false
repository.jdbc.dbtable=licenta.repository
repository.jdbc.user=crow
repository.jdbc.password=crow
